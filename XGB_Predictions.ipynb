{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayNCode/code_collab/blob/main/XGB_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e58hofB62gm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://raw.githubusercontent.com/RayNCode/code_collab/main/project-3-files\""
      ],
      "metadata": {
        "id": "NAH_6GogaNzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_dataset = pd.read_csv(f\"{data_url}/learn_dataset.csv\").copy()\n",
        "learn_dataset_Emp_contract = pd.read_csv(f\"{data_url}/learn_dataset_Emp_contract.csv\").copy()\n",
        "learn_dataset_sport = pd.read_csv(f\"{data_url}/learn_dataset_sport.csv\").copy()\n",
        "learn_dataset_job = pd.read_csv(f\"{data_url}/learn_dataset_job.csv\").copy()\n",
        "\n",
        "code_work_description_map = pd.read_csv(f\"{data_url}/code_work_description_map.csv\").copy()\n",
        "city_adm = pd.read_csv(f\"{data_url}/city_adm.csv\").copy()\n",
        "code_Club = pd.read_csv(f\"{data_url}/code_Club.csv\").copy()\n",
        "departments = pd.read_csv(f\"{data_url}/departments.csv\").copy()\n",
        "\n",
        "test_dataset_job = pd.read_csv(f\"{data_url}/test_dataset_job.csv\").copy()\n",
        "test_dataset = pd.read_csv(f\"{data_url}/test_dataset.csv\").copy()\n",
        "test_dataset_Emp_contract = pd.read_csv(f\"{data_url}/test_dataset_Emp_contract.csv\").copy()\n",
        "test_dataset_sport = pd.read_csv(f\"{data_url}/test_dataset_sport.csv\").copy()"
      ],
      "metadata": {
        "id": "xJcT5tKBaHkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_base(learn_dataset_job, work_desc, learn_dataset, dept_code, emp_contract, learn_dataset_sport, code_club, departments):\n",
        "    # Chargement et fusion des datasets de travail\n",
        "    merged_df = pd.merge(learn_dataset_job, work_desc, left_on='work_description', right_on='N3', how='left')\n",
        "\n",
        "    # Conversion des colonnes N2, N1 et N3 en chaînes de caractères\n",
        "    merged_df['N2'] = merged_df['N2'].astype(str)\n",
        "    merged_df['N1'] = merged_df['N1'].astype(str)\n",
        "    merged_df['N3'] = merged_df['N3'].astype(str)\n",
        "\n",
        "    # Remplir les valeurs manquantes\n",
        "    merged_df['N2'].fillna(merged_df['N3'].str[:-2], inplace=True)\n",
        "    merged_df['N1'].fillna(merged_df['N2'].str[:-1], inplace=True)\n",
        "\n",
        "    # Fusion avec d'autres datasets\n",
        "    data_2 = pd.merge(learn_dataset, merged_df, on=\"Id\", how=\"left\")\n",
        "    df_1 = data_2.merge(dept_code, on='insee_code')\n",
        "    df_2 = df_1.merge(emp_contract, on='Id', how='left')\n",
        "    df_3 = df_2.merge(learn_dataset_sport, on='Id', how='left')\n",
        "    df_4 = df_3.merge(code_Club, left_on='Club', right_on='Code', how='left')\n",
        "    final_df = df_4.merge(departments, on='dep', how='left')\n",
        "\n",
        "    # Conversion de type pour les colonnes 'Categorie' et 'REG'\n",
        "    final_df['Categorie'] = final_df['Categorie'].astype(str)\n",
        "    final_df['Categorie'] = final_df['Categorie'].astype('object')\n",
        "    final_df['REG'] = final_df['REG'].astype(str)\n",
        "    final_df['REG'] = final_df['REG'].astype('object')\n",
        "\n",
        "    # Création et application d'une condition pour filtrer et imputer des valeurs\n",
        "    condition = (final_df['ACTIVITY_TYPE'] != \"TYPE1|1\")\n",
        "    final_df.loc[condition, ['EMOLUMENT', 'Working_hours']] = 0.0\n",
        "    # Traitement des valeurs manquantes dans les colonnes catégorielles\n",
        "    categorical_columns = final_df.select_dtypes(include=['object']).columns\n",
        "    final_df[categorical_columns] = final_df[categorical_columns].fillna(\"None\")\n",
        "\n",
        "\n",
        "    return final_df\n"
      ],
      "metadata": {
        "id": "3_U-oHI37lNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_learn_test(final_df): #ajouter l'ID quelque part\n",
        "    # Séparer la variable cible si elle est présente\n",
        "    if 'target' in final_df.columns:\n",
        "        y = final_df['target'].copy()\n",
        "        y = np.where(y == 'B', 1, 0)\n",
        "        X = final_df.drop(['target'], axis='columns')\n",
        "    else:\n",
        "        X = final_df.copy()\n",
        "        X_test_id = X['Id'].copy()\n",
        "\n",
        "    # Suppression des colonnes non nécessaires\n",
        "    X = X.drop([\"Id\", 'Nom de la commune', 'Nom fédération', 'Nom catégorie', 'Nom du département', 'Code'], axis=\"columns\")\n",
        "\n",
        "\n",
        "    # Imputation des valeurs manquantes - Ce n'est certainement pas la façon la plus efficiente de le faire. À voir si l'on change.\n",
        "    # imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=50, random_state=0)\n",
        "    # columns_to_impute = ['Working_hours', 'EMOLUMENT']\n",
        "    # X[columns_to_impute] = imputer.fit_transform(X[columns_to_impute])\n",
        "\n",
        "    # On devrait essayer avec XGB et voir les performances également.\n",
        "    # imputer = IterativeImputer(estimator=XGBRegressor(), max_iter=50, random_state=0)\n",
        "    # columns_to_impute = ['Working_hours', 'EMOLUMENT']\n",
        "    # X[columns_to_impute] = imputer.fit_transform(X[columns_to_impute])\n",
        "\n",
        "    if 'target' in final_df.columns:\n",
        "      return X, y\n",
        "    else:\n",
        "      return X, X_test_id\n",
        "\n"
      ],
      "metadata": {
        "id": "ldQlzwI2-5PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_dataset_base = preprocess_data_base(learn_dataset_job, code_work_description_map, learn_dataset, city_adm, learn_dataset_Emp_contract, learn_dataset_sport, code_Club, departments)\n",
        "test_dataset_base = preprocess_data_base(test_dataset_job, code_work_description_map, test_dataset, city_adm, test_dataset_Emp_contract, test_dataset_sport, code_Club,departments)"
      ],
      "metadata": {
        "id": "EHI6cVyVAbf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = preprocess_learn_test(learn_dataset_base)\n",
        "X_test, X_test_id = preprocess_learn_test(test_dataset_base)\n",
        "print(\"Shape de X_train:\", X_train.shape)\n",
        "print(\"Shape de y_test:\", y_train.shape)\n",
        "print(\"Shape de X_test:\", X_test.shape)"
      ],
      "metadata": {
        "id": "piqVKOBPicYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mélange de X_train et y_train\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=42)"
      ],
      "metadata": {
        "id": "4sfhR3eeEpY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paramètres pour XGBClassifier\n",
        "xgb_params = {\n",
        "    'subsample': 0.9000000000000001,\n",
        "    'scale_pos_weight': 0.8300000000000001,\n",
        "    'reg_lambda': 0.08,\n",
        "    'reg_alpha': 0.8,\n",
        "    'n_estimators': 151,\n",
        "    'min_child_weight': 3.279999999999994,\n",
        "    'max_depth': 5,\n",
        "    'max_delta_step': 1.54,\n",
        "    'learning_rate': 0.76,\n",
        "    'gamma': 0.9980000000000002,\n",
        "    'colsample_bytree': 0.7100000000000001,\n",
        "    'colsample_bylevel': 0.9400000000000002\n",
        "}"
      ],
      "metadata": {
        "id": "LTA8i6y5JjUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(X_train, X_test):\n",
        "    # Transformer de colonnes pour le prétraitement\n",
        "    preprocessing = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('imputer', IterativeImputer(estimator=RandomForestRegressor(), max_iter=100, random_state=0), ['Working_hours', 'EMOLUMENT']),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'), ['insee_code', \"is_student\", \"OCCUPATION_42\", \"ACTIVITY_TYPE\", \"household\", \"sex\", \"employer_category\", \"job_category\", \"Terms_of_emp\", \"Eco_sect\", \"Job_dep\", \"WORK_CONDITION\", \"work_description\", \"N3\", \"N2\", \"N1\", \"town_type\", \"dep\", \"Emp_contract\", \"Club\", \"Categorie\", 'REG']),\n",
        "            ('ordinal', OrdinalEncoder(), [\"Highest_degree\", \"EMPLOYEE_COUNT\"])\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # Appliquer le prétraitement sur X_train et X_test\n",
        "    X_train_preprocessed = preprocessing.fit_transform(X_train)\n",
        "    X_test_preprocessed = preprocessing.transform(X_test)\n",
        "\n",
        "    xgb_classifier = XGBClassifier(random_state=42, **xgb_params)\n",
        "    xgb_classifier.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "    return  xgb_classifier, preprocessing, X_train_preprocessed, X_test_preprocessed\n",
        "\n",
        "# Utilisation de la fonction\n",
        "xgb_classifier, preprocessing, X_train_preprocessed, X_test_preprocessed = process_data(X_train, X_test)\n"
      ],
      "metadata": {
        "id": "vzlXW-8PsUdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e24697-b436-4f3c-ffda-afcbb1a3c622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = xgb_classifier.predict(X_test_preprocessed)\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Calcul de l'exactitude\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Exactitude (Accuracy): {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Calcul de la précision\n",
        "precision = precision_score(y_test, y_pred)\n",
        "print(\"Précision (Precision): {:.2f}%\".format(precision * 100))\n",
        "\n",
        "# Calcul du rappel\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(\"Rappel (Recall): {:.2f}%\".format(recall * 100))\n",
        "\n",
        "# Calcul du F1-score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"F1-score: {:.2f}\".format(f1))\n",
        "\n",
        "# Matrice de confusion\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matrice de confusion:\\n\", confusion)"
      ],
      "metadata": {
        "id": "MteqDA7C6SM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape de X_test:\", X_train_preprocessed.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJpS3zruiu_R",
        "outputId": "830502cc-0192-46ca-e30e-0aeca5cb8289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de X_test: (49993, 14740)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape de X_test:\", X_test_preprocessed.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coCAG1oTcIY5",
        "outputId": "41053f0b-1ac1-466c-89ae-6b133eaafd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de X_test: (49992, 14740)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Maintenant, X_train_preprocessed est prêt pour l'entraînement, et X_test_preprocessed est prêt pour les prédictions.\n",
        "predictions = xgb_classifier.predict(X_test_preprocessed)"
      ],
      "metadata": {
        "id": "iWcfnyb5bL75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir les prédictions numériques en étiquettes catégorielles\n",
        "predictions_labels = np.where(predictions == 1, 'B', 'G')\n",
        "\n",
        "# Créer un DataFrame avec les Ids et les prédictions\n",
        "results_df = pd.DataFrame({\n",
        "    'Id': X_test_id,\n",
        "    'target': predictions_labels\n",
        "})\n",
        "\n",
        "results_df.to_csv(\"/kaggle/working/predictions.csv\", index=False)\n",
        "# Calcul du décompte des valeurs pour les prédictions\n",
        "value_counts = results_df['target'].value_counts()\n",
        "\n",
        "# Affichage du décompte\n",
        "print(value_counts)\n",
        "\n"
      ],
      "metadata": {
        "id": "4SM_uQ7ybX6D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}