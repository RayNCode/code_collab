{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfQtoM1Gy+fVr0hTRjWMuX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayNCode/code_collab/blob/main/ColumnTransformer_(Digangana).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding my part of the column transformer, even though it doesn't work for me in case you want to have a look. I have added the simple imputer code in the type converison sub pipeline"
      ],
      "metadata": {
        "id": "CLN7p_uw7mPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, classification_report"
      ],
      "metadata": {
        "id": "9UfHs_Aw7lKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://raw.githubusercontent.com/RayNCode/code_collab/main/project-3-files\""
      ],
      "metadata": {
        "id": "OdEdX00px2W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_dataset = pd.read_csv(f\"{data_url}/learn_dataset.csv\").copy()\n",
        "learn_dataset_Emp_contract = pd.read_csv(f\"{data_url}/learn_dataset_Emp_contract.csv\").copy()\n",
        "learn_dataset_sport = pd.read_csv(f\"{data_url}/learn_dataset_sport.csv\").copy()\n",
        "learn_dataset_job = pd.read_csv(f\"{data_url}/learn_dataset_job.csv\").copy()\n",
        "\n",
        "code_work_description_map = pd.read_csv(f\"{data_url}/code_work_description_map.csv\").copy()\n",
        "city_adm = pd.read_csv(f\"{data_url}/city_adm.csv\").copy()\n",
        "code_Club = pd.read_csv(f\"{data_url}/code_Club.csv\").copy()\n",
        "departments = pd.read_csv(f\"{data_url}/departments.csv\").copy()\n",
        "\n",
        "test_dataset_job = pd.read_csv(f\"{data_url}/test_dataset_job.csv\").copy()\n",
        "test_dataset = pd.read_csv(f\"{data_url}/test_dataset.csv\").copy()\n",
        "test_dataset_Emp_contract = pd.read_csv(f\"{data_url}/test_dataset_Emp_contract.csv\").copy()\n",
        "test_dataset_sport = pd.read_csv(f\"{data_url}/test_dataset_sport.csv\").copy()"
      ],
      "metadata": {
        "id": "o8Q2PaDYx2Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_base(learn_dataset_job, work_desc, learn_dataset, dept_code, emp_contract, learn_dataset_sport, code_club, departments):\n",
        "    # Chargement et fusion des datasets de travail\n",
        "    merged_df = pd.merge(learn_dataset_job, work_desc, left_on='work_description', right_on='N3', how='left')\n",
        "\n",
        "    # Conversion des colonnes N2, N1 et N3 en chaînes de caractères\n",
        "    merged_df['N2'] = merged_df['N2'].astype(str)\n",
        "    merged_df['N1'] = merged_df['N1'].astype(str)\n",
        "    merged_df['N3'] = merged_df['N3'].astype(str)\n",
        "\n",
        "    # Remplir les valeurs manquantes\n",
        "    merged_df['N2'].fillna(merged_df['N3'].str[:-2], inplace=True)\n",
        "    merged_df['N1'].fillna(merged_df['N2'].str[:-1], inplace=True)\n",
        "\n",
        "    # Fusion avec d'autres datasets\n",
        "    data_2 = pd.merge(learn_dataset, merged_df, on=\"Id\", how=\"left\")\n",
        "    df_1 = data_2.merge(dept_code, on='insee_code')\n",
        "    df_2 = df_1.merge(emp_contract, on='Id', how='left')\n",
        "    df_3 = df_2.merge(learn_dataset_sport, on='Id', how='left')\n",
        "    df_4 = df_3.merge(code_Club, left_on='Club', right_on='Code', how='left')\n",
        "    final_df = df_4.merge(departments, on='dep', how='left')\n",
        "\n",
        "\n",
        "    return final_df\n"
      ],
      "metadata": {
        "id": "msUHq4D0x2c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_learn_test(final_df): #ajouter l'ID quelque part\n",
        "    # Séparer la variable cible si elle est présente\n",
        "    if 'target' in final_df.columns:\n",
        "        y = final_df['target'].copy()\n",
        "        y = np.where(y == 'B', 1, 0)\n",
        "        X = final_df.drop(['target'], axis='columns')\n",
        "    else:\n",
        "        X = final_df.copy()\n",
        "        X_test_id = X['Id'].copy()\n",
        "\n",
        "    # Suppression des colonnes non nécessaires\n",
        "    X = X.drop([\"Id\", 'Nom de la commune', 'Nom fédération', 'Nom catégorie', 'Nom du département', 'Code'], axis=\"columns\")\n",
        "\n",
        "\n",
        "    if 'target' in final_df.columns:\n",
        "      return X, y\n",
        "    else:\n",
        "      return X, X_test_id\n"
      ],
      "metadata": {
        "id": "TZnFnYmjx2gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_dataset_base = preprocess_data_base(learn_dataset_job, code_work_description_map, learn_dataset, city_adm, learn_dataset_Emp_contract, learn_dataset_sport, code_Club, departments)\n",
        "test_dataset_base = preprocess_data_base(test_dataset_job, code_work_description_map, test_dataset, city_adm, test_dataset_Emp_contract, test_dataset_sport, code_Club, departments)"
      ],
      "metadata": {
        "id": "e6BsCDQRx2mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = preprocess_learn_test(learn_dataset_base)\n",
        "X_testing, X_testing_id = preprocess_learn_test(test_dataset_base)\n",
        "print(\"Shape de X_train:\", X_train.shape)\n",
        "print(\"Shape de y_test:\", y_train.shape)\n",
        "print(\"Shape de X_test:\", X_testing.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwdUhxUsx2pC",
        "outputId": "0611445e-8ac3-40eb-d813-f585aab1e6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de X_train: (49993, 27)\n",
            "Shape de y_test: (49993,)\n",
            "Shape de X_test: (49992, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=0, stratify=y_train)"
      ],
      "metadata": {
        "id": "Qs95TyYJymeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = ['EMOLUMENT', 'Working_hours']\n",
        "recode_features = ['Categorie', 'REG']\n",
        "one_hot_features = ['insee_code', \"is_student\", \"OCCUPATION_42\", \"ACTIVITY_TYPE\", \"household\", \"sex\", \"employer_category\", \"job_category\", \"Terms_of_emp\", \"Eco_sect\", \"Job_dep\", \"WORK_CONDITION\", \"work_description\", \"N3\", \"N2\", \"N1\", \"town_type\", \"dep\", \"Emp_contract\", \"Club\", \"Categorie\", 'REG']\n",
        "\n",
        "\n",
        "#preprocessing = ColumnTransformer(\n",
        "#      transformers=[\n",
        "#            ('type_conversion', Pipeline(steps=[\n",
        "#                ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
        "#                ('convert_to_object', FunctionTransformer(lambda x: x.astype('object'), validate=False))\n",
        "#                ]), recode_features),\n",
        "#            ('custom_condition', Pipeline(steps=[\n",
        "#                ('condition', FunctionTransformer(lambda x: np.where(x != \"TYPE1|1\", x, 0.0), validate=False))\n",
        "#                 ]), numeric_features),\n",
        "#            ('fill_missing_categorical', SimpleImputer(strategy='constant', fill_value='None'), one_hot_features),\n",
        "#            ('imputer', IterativeImputer(estimator=RandomForestRegressor(), max_iter=50, random_state=0), numeric_features),\n",
        "#            ('onehot', OneHotEncoder(handle_unknown='ignore'), one_hot_features),\n",
        "#            ('ordinal', OrdinalEncoder(), [\"Highest_degree\", \"EMPLOYEE_COUNT\"]),\n",
        "#        ],\n",
        "#        remainder='passthrough'\n",
        "#    )\n",
        "\n",
        "preprocessing = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('type_conversion', Pipeline(steps=[\n",
        "            ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
        "            ('convert_to_object', FunctionTransformer(lambda x: x.astype('object'), validate=False))\n",
        "        ]), recode_features),\n",
        "        ('fill_missing_categorical', SimpleImputer(strategy='constant', fill_value='None'), one_hot_features),\n",
        "        ('custom_condition', Pipeline(steps=[\n",
        "            ('condition', FunctionTransformer(lambda x: np.where(x != \"TYPE1|1\", x, 0.0), validate=False))\n",
        "        ]), numeric_features),\n",
        "        ('imputer', IterativeImputer(estimator=RandomForestRegressor(), max_iter=50, random_state=0), numeric_features),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'), one_hot_features),\n",
        "        ('ordinal', OrdinalEncoder(), [\"Highest_degree\", \"EMPLOYEE_COUNT\"]),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "\n",
        "full_pipeline = Pipeline([\n",
        "    (\"preprocessing\", preprocessing),\n",
        "    (\"xgbclassifier\", XGBClassifier(\n",
        "        random_state=42,\n",
        "        tree_method='gpu_hist',  # Use GPU for training\n",
        "        gpu_id=0,  # Specify the GPU ID (0 if you have only one GPU)\n",
        "        predictor='gpu_predictor'  # Specify the GPU predictor\n",
        "    ))\n",
        "])\n",
        "\n",
        "xgb_param_grid = {\n",
        "    'xgbclassifier__subsample': np.arange(0.7, 1.2, 0.01),\n",
        "    'xgbclassifier__scale_pos_weight': np.arange(0.7, 1, 0.01),\n",
        "    'xgbclassifier__reg_lambda': np.arange(0.08, 0.10, 0.01),\n",
        "    'xgbclassifier__reg_alpha': np.arange(0.01, 1, 0.01),\n",
        "    'xgbclassifier__n_estimators': np.arange(100, 200, 1),\n",
        "    'xgbclassifier__min_child_weight': np.arange(3, 4, 0.01),\n",
        "    'xgbclassifier__max_depth': np.arange(4, 6, 1),\n",
        "    'xgbclassifier__max_delta_step': np.arange(1.5, 2, 0.01),\n",
        "    'xgbclassifier__learning_rate': np.arange(0.7, 1, 0.01),\n",
        "    'xgbclassifier__gamma': np.arange(0.8, 1, 0.01),\n",
        "    'xgbclassifier__colsample_bytree': np.arange(0.65, 0.75, 0.01),\n",
        "    'xgbclassifier__colsample_bylevel': np.arange(0.8, 1, 0.01),\n",
        "    }\n",
        "\n",
        "# full_pipeline\n",
        "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "# grid_search = RandomizedSearchCV(\n",
        "#        estimator=full_pipeline,\n",
        "#        param_distributions=xgb_param_grid,\n",
        "#        cv=cv,\n",
        "#        scoring='accuracy',\n",
        "#        n_iter=50,\n",
        "#        n_jobs=-1)\n",
        "\n",
        "# grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "UR7JC2fUymia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iD0HwqF1G-ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "1oacJF4gGrt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip show xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXshUFaFEAuj",
        "outputId": "2c4cf0ad-609f-4af9-854a-600497c69e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: xgboost\n",
            "Version: 2.0.3\n",
            "Summary: XGBoost Python Package\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Hyunsu Cho <chohyu01@cs.washington.edu>, Jiaming Yuan <jm.yuan@outlook.com>\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy, scipy\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#xgb_param_grid = {\n",
        "#    'xgbclassifier__subsample': 0.9000000000000001,\n",
        "#    'xgbclassifier__scale_pos_weight': 0.8300000000000001,\n",
        "#    'xgbclassifier__reg_lambda': 0.08,\n",
        "#    'xgbclassifier__reg_alpha': 0.8,\n",
        "#    'xgbclassifier__n_estimators': 151,\n",
        "#    'xgbclassifier__min_child_weight': 3.279999999999994,\n",
        "#    'xgbclassifier__max_depth': 5,\n",
        "#    'xgbclassifier__max_delta_step': 1.54,\n",
        "#    'xgbclassifier__learning_rate': 0.76,\n",
        "#    'xgbclassifier__gamma': 0.9980000000000002,\n",
        "#    'xgbclassifier__colsample_bytree': 0.7100000000000001,\n",
        "#    'xgbclassifier__colsample_bylevel': 0.9400000000000002}"
      ],
      "metadata": {
        "id": "A4iS45Iz_clZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_hyperparameters(pipeline, param_grid, X_train, y_train):\n",
        "    \"\"\"\n",
        "    This function tunes the hyperparameters of a classifier using GridSearchCV and cross-validation\n",
        "    and returns the best classifier model with the optimal hyperparameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the cross-validation object using StratifiedKFold to ensure the class distribution is the same across all the folds\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "    # Create the RandomizedSearch object\n",
        "    clf_grid = RandomizedSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_distributions=param_grid,\n",
        "        cv=cv, scoring='accuracy',\n",
        "        n_iter=25,\n",
        "        n_jobs=-1)\n",
        "\n",
        "    # Fit the GridSearchCV object to the training data\n",
        "    clf_grid.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best hyperparameters\n",
        "    print(\"Best hyperparameters:\\n\", clf_grid.best_params_)\n",
        "\n",
        "    # Return best_estimator_ attribute which gives us the best model that has been fitted to the training data\n",
        "    return clf_grid.best_estimator_\n"
      ],
      "metadata": {
        "id": "LwTap7ZNymle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_opt = tune_hyperparameters(full_pipeline, xgb_param_grid, X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cjeteaWbymos",
        "outputId": "1af68ea6-5476-4fa8-99b8-90cf950c66df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\nAll the 125 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n100 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 830, in _hstack\n    converted_Xs = [\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 831, in <listcomp>\n    check_array(X, accept_sparse=True, force_all_finite=False)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: '2B152'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 751, in fit_transform\n    return self._hstack(list(Xs))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 835, in _hstack\n    raise ValueError(\nValueError: For a sparse output, all columns should be a numeric or convertible to a numeric.\n\n--------------------------------------------------------------------------------\n25 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 830, in _hstack\n    converted_Xs = [\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 831, in <listcomp>\n    check_array(X, accept_sparse=True, force_all_finite=False)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: '2B033'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 751, in fit_transform\n    return self._hstack(list(Xs))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 835, in _hstack\n    raise ValueError(\nValueError: For a sparse output, all columns should be a numeric or convertible to a numeric.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-1edf10138df2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-cdb7ae9a8a50>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(pipeline, param_grid, X_train, y_train)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Fit the GridSearchCV object to the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mclf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Get the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 125 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n100 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 830, in _hstack\n    converted_Xs = [\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 831, in <listcomp>\n    check_array(X, accept_sparse=True, force_all_finite=False)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: '2B152'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 751, in fit_transform\n    return self._hstack(list(Xs))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 835, in _hstack\n    raise ValueError(\nValueError: For a sparse output, all columns should be a numeric or convertible to a numeric.\n\n--------------------------------------------------------------------------------\n25 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 830, in _hstack\n    converted_Xs = [\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 831, in <listcomp>\n    check_array(X, accept_sparse=True, force_all_finite=False)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: '2B033'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 751, in fit_transform\n    return self._hstack(list(Xs))\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 835, in _hstack\n    raise ValueError(\nValueError: For a sparse output, all columns should be a numeric or convertible to a numeric.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_calculator(clf, X_test, y_test, model_name):\n",
        "    '''\n",
        "    This function calculates all desired performance metrics for a given model on test data.\n",
        "    '''\n",
        "    y_pred = clf.predict(X_test)\n",
        "    result = pd.DataFrame(data=[accuracy_score(y_test, y_pred),\n",
        "                                precision_score(y_test, y_pred, average='macro'),\n",
        "                                recall_score(y_test, y_pred, average='macro'),\n",
        "                                f1_score(y_test, y_pred, average='macro'),\n",
        "                                roc_auc_score(y_test, clf.predict_proba(X_test)[::,1], average='macro')],\n",
        "                          index=['Accuracy','Macro Precision','Macro Recall','Macro F1-score','Macro AUC'],\n",
        "                          columns = [model_name])\n",
        "\n",
        "    result = (result * 100).round(2).astype(str) + '%'\n",
        "    return result"
      ],
      "metadata": {
        "id": "XCR7ykgVymrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(clf, X_train, X_test, y_train, y_test, model_name):\n",
        "    '''\n",
        "    This function provides a complete report of the model's performance including classification reports and confusion matrix\n",
        "    '''\n",
        "    # Set font scale\n",
        "    sns.set(font_scale=1.5)\n",
        "\n",
        "    # Generate classification report for training set\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    print(\"\\n\\t  Classification report for training set\")\n",
        "    print(\"-\"*55)\n",
        "    print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "    # Generate classification report for test set\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "    print(\"\\n\\t   Classification report for test set\")\n",
        "    print(\"-\"*55)\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "    print('\\n')\n",
        "\n",
        "    # Create figure and subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, dpi=90, figsize=(12, 5))\n",
        "\n",
        "    # Plot confusion matrix for test set\n",
        "    cmap = plt.cm.Purples  # Remplace 'purple_cmap' par 'cmap'\n",
        "\n",
        "    ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, colorbar=False, cmap=cmap, ax=ax1)\n",
        "    ax1.set_title('Confusion Matrix for Test Data')\n",
        "    ax1.grid(False)\n",
        "\n",
        "    # Report desired results as a summary in the form of a table\n",
        "    result = metrics_calculator(clf, X_test, y_test, model_name)\n",
        "    table = ax2.table(cellText=result.values, colLabels=result.columns, rowLabels=result.index, loc='center')\n",
        "    table.scale(0.6, 3.6)\n",
        "    table.set_fontsize(12)\n",
        "    ax2.axis('tight')\n",
        "    # Hide the axes\n",
        "    ax2.axis('off')\n",
        "    # set the title\n",
        "    ax2.set_title('{} Performance Summary on Test Data'.format(model_name), fontsize=18)\n",
        "    # Modify color\n",
        "    for key, cell in table.get_celld().items():\n",
        "        if key[0] == 0:\n",
        "          cell.set_color('purple')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ENcWAnkPymxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_evaluation(xgb_opt, X_train, X_test, y_train, y_test, 'XGBoost')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "X1ucJMIVx2rs",
        "outputId": "ab66cebd-a467-4e05-8ebf-9197ba4e808d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'xgb_opt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-a49cc0405c33>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'XGBoost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'xgb_opt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwNNJVK9x2uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3zTOlFax2xG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}