{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4vyUiQlhXG51nSPitE6Q4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayNCode/code_collab/blob/main/XGB_future_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xCvn1lWClEiN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://raw.githubusercontent.com/RayNCode/code_collab/main/project-3-files\""
      ],
      "metadata": {
        "id": "TXtrFU4xlNkC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_dataset = pd.read_csv(f\"{data_url}/learn_dataset.csv\").copy()\n",
        "learn_dataset_Emp_contract = pd.read_csv(f\"{data_url}/learn_dataset_Emp_contract.csv\").copy()\n",
        "learn_dataset_sport = pd.read_csv(f\"{data_url}/learn_dataset_sport.csv\").copy()\n",
        "learn_dataset_job = pd.read_csv(f\"{data_url}/learn_dataset_job.csv\").copy()\n",
        "\n",
        "code_work_description_map = pd.read_csv(f\"{data_url}/code_work_description_map.csv\").copy()\n",
        "city_adm = pd.read_csv(f\"{data_url}/city_adm.csv\").copy()\n",
        "code_Club = pd.read_csv(f\"{data_url}/code_Club.csv\").copy()\n",
        "departments = pd.read_csv(f\"{data_url}/departments.csv\").copy()\n",
        "\n",
        "test_dataset_job = pd.read_csv(f\"{data_url}/test_dataset_job.csv\").copy()\n",
        "test_dataset = pd.read_csv(f\"{data_url}/test_dataset.csv\").copy()\n",
        "test_dataset_Emp_contract = pd.read_csv(f\"{data_url}/test_dataset_Emp_contract.csv\").copy()\n",
        "test_dataset_sport = pd.read_csv(f\"{data_url}/test_dataset_sport.csv\").copy()"
      ],
      "metadata": {
        "id": "fjFuDEd7lNmh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_base(learn_dataset_job, work_desc, learn_dataset, dept_code, emp_contract, learn_dataset_sport, code_club, departments):\n",
        "    # Chargement et fusion des datasets de travail\n",
        "    merged_df = pd.merge(learn_dataset_job, work_desc, left_on='work_description', right_on='N3', how='left')\n",
        "\n",
        "    # Conversion des colonnes N2, N1 et N3 en chaînes de caractères\n",
        "    merged_df['N2'] = merged_df['N2'].astype(str)\n",
        "    merged_df['N1'] = merged_df['N1'].astype(str)\n",
        "    merged_df['N3'] = merged_df['N3'].astype(str)\n",
        "\n",
        "    # Remplir les valeurs manquantes\n",
        "    merged_df['N2'].fillna(merged_df['N3'].str[:-2], inplace=True)\n",
        "    merged_df['N1'].fillna(merged_df['N2'].str[:-1], inplace=True)\n",
        "\n",
        "    # Fusion avec d'autres datasets\n",
        "    data_2 = pd.merge(learn_dataset, merged_df, on=\"Id\", how=\"left\")\n",
        "    df_1 = data_2.merge(dept_code, on='insee_code')\n",
        "    df_2 = df_1.merge(emp_contract, on='Id', how='left')\n",
        "    df_3 = df_2.merge(learn_dataset_sport, on='Id', how='left')\n",
        "    df_4 = df_3.merge(code_Club, left_on='Club', right_on='Code', how='left')\n",
        "    final_df = df_4.merge(departments, on='dep', how='left')\n",
        "\n",
        "    # Conversion de type pour les colonnes 'Categorie' et 'REG'\n",
        "    final_df['Categorie'] = final_df['Categorie'].astype(str)\n",
        "    final_df['Categorie'] = final_df['Categorie'].astype('object')\n",
        "    final_df['REG'] = final_df['REG'].astype(str)\n",
        "    final_df['REG'] = final_df['REG'].astype('object')\n",
        "\n",
        "    # Création et application d'une condition pour filtrer et imputer des valeurs\n",
        "    condition = (final_df['ACTIVITY_TYPE'] != \"TYPE1|1\")\n",
        "    final_df.loc[condition, ['EMOLUMENT', 'Working_hours']] = 0.0\n",
        "    # Traitement des valeurs manquantes dans les colonnes catégorielles\n",
        "    categorical_columns = final_df.select_dtypes(include=['object']).columns\n",
        "    final_df[categorical_columns] = final_df[categorical_columns].fillna(\"None\")\n",
        "\n",
        "\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "YZ8iQKcplNpH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_learn_test(final_df):\n",
        "    if 'target' in final_df.columns:\n",
        "        y = final_df['target'].copy()\n",
        "        y = np.where(y == 'B', 1, 0)\n",
        "        X = final_df.drop(['target'], axis='columns')\n",
        "    else:\n",
        "        X = final_df.copy()\n",
        "        X_test_id = X['Id'].copy()\n",
        "\n",
        "    X = X.drop([\"Id\", 'Nom de la commune', 'Nom fédération', 'Nom catégorie', 'Nom du département', 'Code'], axis=\"columns\")\n",
        "\n",
        "\n",
        "    # Imputation des valeurs manquantes\n",
        "    imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=50, random_state=0)\n",
        "    columns_to_impute = ['Working_hours', 'EMOLUMENT']\n",
        "    X[columns_to_impute] = imputer.fit_transform(X[columns_to_impute])\n",
        "\n",
        "\n",
        "    if 'target' in final_df.columns:\n",
        "      return X, y\n",
        "    else:\n",
        "      return X, X_test_id"
      ],
      "metadata": {
        "id": "2B79mK0-lNrr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_dataset_base = preprocess_data_base(learn_dataset_job, code_work_description_map, learn_dataset, city_adm, learn_dataset_Emp_contract, learn_dataset_sport, code_Club, departments)\n",
        "test_dataset_base = preprocess_data_base(test_dataset_job, code_work_description_map, test_dataset, city_adm, test_dataset_Emp_contract, test_dataset_sport, code_Club,departments)"
      ],
      "metadata": {
        "id": "d014EFt7lNty"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = preprocess_learn_test(learn_dataset_base) #X_train, y_train\n",
        "X_test, X_test_id = preprocess_learn_test(test_dataset_base)\n",
        "print(\"Shape de X_train:\", X_train.shape)\n",
        "print(\"Shape de y_test:\", y_train.shape)\n",
        "print(\"Shape de X_test:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-uAMqHDlNwH",
        "outputId": "7d6a2377-3c4a-4309-8a62-b35ad8333607"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de X_train: (49993, 27)\n",
            "Shape de y_test: (49993,)\n",
            "Shape de X_test: (49992, 27)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
      ],
      "metadata": {
        "id": "l_Sb-k0PlNyw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Meilleurs Paramètres pour XGBClassifier\n",
        "xgb_params = {\n",
        "    'subsample': 0.9000000000000001,\n",
        "    'scale_pos_weight': 0.8300000000000001,\n",
        "    'reg_lambda': 0.08,\n",
        "    'reg_alpha': 0.8,\n",
        "    'n_estimators': 151,\n",
        "    'min_child_weight': 3.279999999999994,\n",
        "    'max_depth': 5,\n",
        "    'max_delta_step': 1.54,\n",
        "    'learning_rate': 0.76,\n",
        "    'gamma': 0.9980000000000002,\n",
        "    'colsample_bytree': 0.7100000000000001,\n",
        "    'colsample_bylevel': 0.9400000000000002\n",
        "}"
      ],
      "metadata": {
        "id": "ASXebJv2mRO9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(X_train, X_test):\n",
        "    # Transformer de colonnes pour le prétraitement\n",
        "    preprocessing = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('imputer', IterativeImputer(estimator=RandomForestRegressor(), max_iter=100, random_state=0), ['Working_hours', 'EMOLUMENT']),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'), ['insee_code', \"is_student\", \"OCCUPATION_42\", \"ACTIVITY_TYPE\", \"household\", \"sex\", \"employer_category\", \"job_category\", \"Terms_of_emp\", \"Eco_sect\", \"Job_dep\", \"WORK_CONDITION\", \"work_description\", \"N3\", \"N2\", \"N1\", \"town_type\", \"dep\", \"Emp_contract\", \"Club\", \"Categorie\", 'REG']),\n",
        "            ('ordinal', OrdinalEncoder(), [\"Highest_degree\", \"EMPLOYEE_COUNT\"])\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # Appliquer le prétraitement sur X_train et X_test\n",
        "    X_train_preprocessed = preprocessing.fit_transform(X_train)\n",
        "    X_test_preprocessed = preprocessing.transform(X_test)\n",
        "\n",
        "    xgb_classifier = XGBClassifier(random_state=42, **xgb_params)\n",
        "    xgb_classifier.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "    return  xgb_classifier, preprocessing, X_train_preprocessed, X_test_preprocessed\n",
        "\n",
        "# Utilisation de la fonction\n",
        "xgb_classifier, preprocessing, X_train_preprocessed, X_test_preprocessed = process_data(X_train, X_test)"
      ],
      "metadata": {
        "id": "alIKtqRSmRR7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y_test, predictions)\n",
        "r_squared = r2_score(y_test, predictions)\n",
        "\n",
        "#Evaluation metrics\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"R-Squared: {r_squared}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDk5ZomnlN77",
        "outputId": "d4dc66c5-9502-4e4c-b73f-fd95c315557c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.0884088408840884\n",
            "R-Squared: 0.6267350006997308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not the most efficient code. Ideally, this should be in XGB_final_model.ipynb. But I did not have enough time to just add the last part of the code due to the hyperparameters computation. Tbh, for me its okay if you want to add this separately or if you have the time to add this last bit to the final_model code. Let me know. Otherwise we can submit the entire thing."
      ],
      "metadata": {
        "id": "4mSx2nfMvZlx"
      }
    }
  ]
}