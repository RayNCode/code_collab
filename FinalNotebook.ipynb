{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJDqooS3NPdLlujPJYwMl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayNCode/code_collab/blob/main/FinalNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7e58hofB62gm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_base(learn_dataset_job_path, work_desc_path, learn_dataset_path, dept_code_path, emp_contract_path, learn_dataset_sport_path, code_club_path, departments_path):\n",
        "    # Chargement et fusion des datasets de travail\n",
        "    learn_dataset_job = pd.read_csv(learn_dataset_job_path)\n",
        "    work_desc = pd.read_csv(work_desc_path)\n",
        "    merged_df = pd.merge(learn_dataset_job, work_desc, left_on='work_description', right_on='N3', how='left')\n",
        "\n",
        "    # Conversion des colonnes N2, N1 et N3 en chaînes de caractères\n",
        "    merged_df['N2'] = merged_df['N2'].astype(str)\n",
        "    merged_df['N1'] = merged_df['N1'].astype(str)\n",
        "    merged_df['N3'] = merged_df['N3'].astype(str)\n",
        "\n",
        "    # Remplir les valeurs manquantes\n",
        "    merged_df['N2'].fillna(merged_df['N3'].str[:-2], inplace=True)\n",
        "    merged_df['N1'].fillna(merged_df['N2'].str[:-1], inplace=True)\n",
        "\n",
        "    # Fusion avec d'autres datasets\n",
        "    learn_dataset = pd.read_csv(learn_dataset_path)\n",
        "    data_2 = pd.merge(learn_dataset, merged_df, on=\"Id\", how=\"left\")\n",
        "    dept_code = pd.read_csv(dept_code_path)\n",
        "    df_1 = data_2.merge(dept_code, on='insee_code')\n",
        "    emp_contract = pd.read_csv(emp_contract_path)\n",
        "    df_2 = df_1.merge(emp_contract, on='Id', how='left')\n",
        "    learn_dataset_sport = pd.read_csv(learn_dataset_sport_path)\n",
        "    df_3 = df_2.merge(learn_dataset_sport, on='Id', how='left')\n",
        "    code_Club = pd.read_csv(code_club_path)\n",
        "    df_4 = df_3.merge(code_Club, left_on='Club', right_on='Code', how='left')\n",
        "    dep = pd.read_csv(departments_path)\n",
        "    final_df = df_4.merge(dep, on='dep', how='left')\n",
        "\n",
        "    # Conversion de type pour les colonnes 'Categorie' et 'REG'\n",
        "    final_df['Categorie'] = final_df['Categorie'].astype(str)\n",
        "    final_df['Categorie'] = final_df['Categorie'].astype('object')\n",
        "    final_df['REG'] = final_df['REG'].astype(str)\n",
        "    final_df['REG'] = final_df['REG'].astype('object')\n",
        "\n",
        "    # Création et application d'une condition pour filtrer et imputer des valeurs\n",
        "    condition = (final_df['ACTIVITY_TYPE'] != \"TYPE1|1\")\n",
        "    final_df.loc[condition, ['EMOLUMENT', 'Working_hours']] = 0.0\n",
        "    # Traitement des valeurs manquantes dans les colonnes catégorielles\n",
        "    categorical_columns = final_df.select_dtypes(include=['object']).columns\n",
        "    final_df[categorical_columns] = final_df[categorical_columns].fillna(\"None\")\n",
        "\n",
        "\n",
        "    return final_df\n"
      ],
      "metadata": {
        "id": "3_U-oHI37lNP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_learn_test(final_df): #ajouter l'ID quelque part\n",
        "    # Séparer la variable cible si elle est présente\n",
        "    if 'target' in final_df.columns:\n",
        "        y = final_df['target'].copy()\n",
        "        y = np.where(y == 'B', 1, 0)\n",
        "        X = final_df.drop(['target'], axis='columns')\n",
        "    else:\n",
        "        X = final_df.copy()\n",
        "        X_test_id = X['Id'].copy()\n",
        "\n",
        "    # Suppression des colonnes non nécessaires\n",
        "    X = X.drop([\"Id\", 'Nom de la commune', 'Nom fédération', 'Nom catégorie', 'Nom du département', 'Code'], axis=\"columns\")\n",
        "\n",
        "\n",
        "    # Imputation des valeurs manquantes - Ce n'est certainement pas la façon la plus efficiente de le faire. À voir si l'on change.\n",
        "    # imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=50, random_state=0)\n",
        "    # columns_to_impute = ['Working_hours', 'EMOLUMENT']\n",
        "    # X[columns_to_impute] = imputer.fit_transform(X[columns_to_impute])\n",
        "\n",
        "    # On devrait essayer avec XGB et voir les performances également.\n",
        "    # imputer = IterativeImputer(estimator=XGBRegressor(), max_iter=50, random_state=0)\n",
        "    # columns_to_impute = ['Working_hours', 'EMOLUMENT']\n",
        "    # X[columns_to_impute] = imputer.fit_transform(X[columns_to_impute])\n",
        "\n",
        "    if 'target' in final_df.columns:\n",
        "      return X, y\n",
        "    else:\n",
        "      return X, X_test_id\n"
      ],
      "metadata": {
        "id": "ldQlzwI2-5PV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_dataset_base = preprocess_data_base(\"/content/learn_dataset_job.csv\",\n",
        "                                      \"/content/code_work_description_map.csv\",\n",
        "                                      \"/content/learn_dataset.csv\",\n",
        "                                      \"/content/city_adm.csv\",\n",
        "                                      \"/content/learn_dataset_Emp_contract.csv\",\n",
        "                                      \"/content/learn_dataset_sport.csv\",\n",
        "                                      \"/content/code_Club.csv\",\n",
        "                                      \"/content/departments.csv\")\n",
        "test_dataset_base = preprocess_data_base(\"/content/test_dataset_job.csv\",\n",
        "                                     \"/content/code_work_description_map.csv\",\n",
        "                                     \"/content/test_dataset.csv\",\n",
        "                                     \"/content/city_adm.csv\",\n",
        "                                     \"/content/test_dataset_Emp_contract.csv\",\n",
        "                                     \"/content/test_dataset_sport.csv\",\n",
        "                                     \"/content/code_Club.csv\",\n",
        "                                     \"/content/departments.csv\")"
      ],
      "metadata": {
        "id": "EHI6cVyVAbf1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = preprocess_learn_test(learn_dataset_base)\n",
        "X_test, X_test_id = preprocess_learn_test(test_dataset_base)\n",
        "print(\"Shape de X_train:\", X_train.shape)\n",
        "print(\"Shape de y_test:\", y_train.shape)\n",
        "print(\"Shape de X_test:\", X_test.shape)\n",
        "\n",
        "print(\"Valeurs nulles dans X_train:\")\n",
        "print(X_train.isnull().sum())\n",
        "\n",
        "print(\"\\nValeurs nulles dans X_test:\")\n",
        "print(X_test.isnull().sum())"
      ],
      "metadata": {
        "id": "piqVKOBPicYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Mélange de X_train et y_train\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=42)"
      ],
      "metadata": {
        "id": "Wx8eaOewGmzj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paramètres pour XGBClassifier\n",
        "xgb_params = {\n",
        "    'subsample': 0.9000000000000001,\n",
        "    'scale_pos_weight': 0.8300000000000001,\n",
        "    'reg_lambda': 0.08,\n",
        "    'reg_alpha': 0.8,\n",
        "    'n_estimators': 151,\n",
        "    'min_child_weight': 3.279999999999994,\n",
        "    'max_depth': 5,\n",
        "    'max_delta_step': 1.54,\n",
        "    'learning_rate': 0.76,\n",
        "    'gamma': 0.9980000000000002,\n",
        "    'colsample_bytree': 0.7100000000000001,\n",
        "    'colsample_bylevel': 0.9400000000000002\n",
        "}"
      ],
      "metadata": {
        "id": "LTA8i6y5JjUI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def process_data(X_train, X_test):\n",
        "    # Transformer de colonnes pour le prétraitement\n",
        "    preprocessing = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('imputer', IterativeImputer(estimator=RandomForestRegressor(), max_iter=50, random_state=0), ['Working_hours', 'EMOLUMENT']),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'), ['insee_code', \"is_student\", \"OCCUPATION_42\", \"ACTIVITY_TYPE\", \"household\", \"sex\", \"employer_category\", \"job_category\", \"Terms_of_emp\", \"Eco_sect\", \"Job_dep\", \"WORK_CONDITION\", \"work_description\", \"N3\", \"N2\", \"N1\", \"town_type\", \"dep\", \"Emp_contract\", \"Club\", \"Categorie\", 'REG']),\n",
        "            ('ordinal', OrdinalEncoder(), [\"Highest_degree\", \"EMPLOYEE_COUNT\"])\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # Appliquer le prétraitement sur X_train et X_test\n",
        "    X_train_preprocessed = preprocessing.fit_transform(X_train)\n",
        "    X_test_preprocessed = preprocessing.transform(X_test)\n",
        "\n",
        "    # Créer et entraîner le modèle XGBClassifier sur les données d'entraînement prétraitées\n",
        "    xgb_classifier = XGBClassifier(random_state=42, **xgb_params)\n",
        "    xgb_classifier.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "    return xgb_classifier, preprocessing, X_train_preprocessed, X_test_preprocessed\n",
        "\n",
        "# Utilisation de la fonction\n",
        "xgb_classifier, preprocessing, X_train_preprocessed, X_test_preprocessed = process_data(X_train, X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "vzlXW-8PsUdg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maintenant, X_train_preprocessed est prêt pour l'entraînement, et X_test_preprocessed est prêt pour les prédictions.\n",
        "predictions = xgb_classifier.predict(X_test_preprocessed)"
      ],
      "metadata": {
        "id": "1bMuOaeNQYIz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_labels = np.where(predictions == 1, 'B', 'G')"
      ],
      "metadata": {
        "id": "IZTWomc8M8wy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir les prédictions numériques en étiquettes catégorielles\n",
        "predictions_labels = np.where(predictions == 1, 'B', 'G')\n",
        "\n",
        "# Créer un DataFrame avec les Ids et les prédictions\n",
        "results_df = pd.DataFrame({\n",
        "    'Id': X_test_id,\n",
        "    'Prediction': predictions_labels\n",
        "})\n"
      ],
      "metadata": {
        "id": "PTRy0bq1cRfY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv(\"/content/predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "Lwhgg6Vfu0VM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul du décompte des valeurs pour les prédictions\n",
        "value_counts = results_df['Prediction'].value_counts()\n",
        "\n",
        "# Affichage du décompte\n",
        "print(value_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAMtEg8HmHZS",
        "outputId": "5ea74050-d70e-4f3a-9c70-5a9711add339"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B    30388\n",
            "G    19604\n",
            "Name: Prediction, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Création du transformateur de colonnes pour le prétraitement\n",
        "# preprocessing = make_column_transformer(\n",
        "#     (OneHotEncoder(handle_unknown='ignore'), ['insee_code', \"is_student\", \"OCCUPATION_42\", \"ACTIVITY_TYPE\", \"household\", \"sex\", \"employer_category\", \"job_category\", \"Terms_of_emp\", \"Eco_sect\", \"Job_dep\", \"WORK_CONDITION\", \"work_description\", \"N3\", \"N2\", \"N1\", \"town_type\", \"dep\", \"Emp_contract\", \"Club\", \"Categorie\", 'REG']),\n",
        "#     (OrdinalEncoder(), [\"Highest_degree\", \"EMPLOYEE_COUNT\"]),\n",
        "#     (IterativeImputer(estimator=RandomForestRegressor(), max_iter=50, random_state=0), ['Working_hours', 'EMOLUMENT']),\n",
        "#     remainder='passthrough'\n",
        "# )\n",
        "\n",
        "# # Création de l'instance XGBClassifier avec les paramètres\n",
        "# xgb_classifier = XGBClassifier(random_state=42, **xgb_params)\n",
        "\n",
        "\n",
        "# # Création du pipeline\n",
        "# pipeline = Pipeline(steps=[\n",
        "#     # ('imputer', imputer),\n",
        "#     ('preprocessor', preprocessing),\n",
        "#     ('classifier', xgb_classifier)\n",
        "# ])\n",
        "# pipeline"
      ],
      "metadata": {
        "id": "h96gEJUWl-Pz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def process_data(data, for_training=True):\n",
        "#     # Transformer de colonnes pour le prétraitement\n",
        "#     preprocessing = make_column_transformer(\n",
        "#         (IterativeImputer(estimator=RandomForestRegressor(), max_iter=50, random_state=0), ['Working_hours', 'EMOLUMENT']),\n",
        "#         (OneHotEncoder(handle_unknown='ignore'), ['insee_code', \"is_student\", \"OCCUPATION_42\", \"ACTIVITY_TYPE\", \"household\", \"sex\", \"employer_category\", \"job_category\", \"Terms_of_emp\", \"Eco_sect\", \"Job_dep\", \"WORK_CONDITION\", \"work_description\", \"N3\", \"N2\", \"N1\", \"town_type\", \"dep\", \"Emp_contract\", \"Club\", \"Categorie\", 'REG']),\n",
        "#         (OrdinalEncoder(), [\"Highest_degree\", \"EMPLOYEE_COUNT\"]),\n",
        "#         remainder='passthrough'\n",
        "#     )\n",
        "\n",
        "#     if for_training:\n",
        "#         # Création de l'instance XGBClassifier avec les paramètres\n",
        "#         xgb_classifier = XGBClassifier(random_state=42, **xgb_params)\n",
        "\n",
        "#         # Pipeline pour l'entraînement\n",
        "#         pipeline = Pipeline([\n",
        "#             ('preprocessor', preprocessing),\n",
        "#             ('classifier', xgb_classifier)\n",
        "#         ])\n",
        "\n",
        "#         return pipeline.fit(data[0], data[1])\n",
        "\n",
        "#     else:\n",
        "#         # Pipeline pour le traitement des données de test\n",
        "#         pipeline = Pipeline([\n",
        "#             ('preprocessor', preprocessing)\n",
        "#         ])\n",
        "\n",
        "#         return pipeline.fit_transform(data)\n",
        "\n",
        "# pipeline = process_data((X_train, y_train), for_training=True)\n",
        "# X_test_transformed = process_data(X_test, for_training=False)\n"
      ],
      "metadata": {
        "id": "kpEkkoSxGhrS"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}